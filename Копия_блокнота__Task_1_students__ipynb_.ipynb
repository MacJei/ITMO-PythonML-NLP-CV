{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUuyLVDZ4s8r"
      },
      "source": [
        "## ЗАДАНИЕ ДЛЯ ПРОГРАММИРУЮЩИХ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUnOinMxn5RT",
        "outputId": "579ffbd2-c777-47de-b49e-7b8288f640dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: numpy\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EZhXZGOn5U3",
        "outputId": "f52d8302-fdb9-4361-a5ee-f1bd7176340c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tqdm\n",
            "Version: 4.45.0\n",
            "Summary: Fast, Extensible Progress Meter\n",
            "Home-page: https://github.com/tqdm/tqdm\n",
            "Author: \n",
            "Author-email: \n",
            "License: MPLv2.0, MIT Licences\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: \n",
            "Required-by: cmdstanpy, fa2, gdown, hyperopt, kaggle, moviepy, nltk, panel, prophet, rnnmorph, spacy, tensorflow-datasets, torchtext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4fUkTXyoPxD",
        "outputId": "6502b2e2-0196-4add-e17c-d4b5780880e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.20.0\n",
            "Uninstalling numpy-1.20.0:\n",
            "  Successfully uninstalled numpy-1.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfjpEnmPoYSu",
        "outputId": "a2d208ff-a920-4324-a53d-01cd14327e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: numpy\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tqdm -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5vu7cCMoP1U",
        "outputId": "5c1fcb36-d4bb-4648-f121-5ec6d27ecebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tqdm 4.45.0\n",
            "Uninstalling tqdm-4.45.0:\n",
            "  Successfully uninstalled tqdm-4.45.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnmWRdRJobyv",
        "outputId": "785c9c84-650e-4884-dcf2-ff0db41c3b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: numpy\n",
            "Version: 1.21.0\n",
            "Summary: NumPy is the fundamental package for array computing with Python.\n",
            "Home-page: https://www.numpy.org\n",
            "Author: Travis E. Oliphant et al.\n",
            "Author-email: \n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: \n",
            "Required-by: aeppl, aesara, albumentations, altair, arviz, astropy, atari-py, autograd, blis, bokeh, cftime, cmdstanpy, cufflinks, cvxpy, daft, datascience, db-dtypes, ecos, fa2, fastdtw, fix-yahoo-finance, folium, gensim, gym, h5py, holoviews, httpstan, hyperopt, imageio, imbalanced-learn, imgaug, jax, jaxlib, jpeg4py, kapre, librosa, lightgbm, matplotlib, matplotlib-venn, missingno, mizani, mlxtend, moviepy, netCDF4, nibabel, numba, numexpr, opencv-contrib-python, opencv-python, opencv-python-headless, opt-einsum, osqp, pandas, pandas-gbq, patsy, plotnine, prophet, pyarrow, pycocotools, pyemd, pyerfa, pymc, pystan, python-louvain, PyWavelets, qdldl, qudida, resampy, rnnmorph, scikit-image, scikit-learn, scipy, scs, seaborn, shapely, sklearn-pandas, spacy, statsmodels, tables, tensorboard, tensorflow, tensorflow-datasets, tensorflow-hub, tensorflow-probability, thinc, tifffile, torchtext, torchvision, wordcloud, xarray, xarray-einstats, xgboost, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.21"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "scO9hij3oezo",
        "outputId": "b16865cc-e772-481e-ab81-8b1ce08e13b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21\n",
            "  Downloading numpy-1.21.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.20.0\n",
            "    Uninstalling numpy-1.20.0:\n",
            "      Successfully uninstalled numpy-1.20.0\n",
            "Successfully installed numpy-1.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whereis python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R2ZO0VqqIDu",
        "outputId": "669f5875-ddf9-4ea7-8b60-95ff66e49586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: /usr/bin/python3.8-config /usr/bin/python2.7 /usr/bin/python3.8 /usr/lib/python2.7 /usr/lib/python3.8 /usr/lib/python3.9 /etc/python2.7 /etc/python3.8 /usr/local/bin/python /usr/local/lib/python3.8 /usr/local/lib/python2.7 /usr/include/python3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "IPJA6sDpqJZX",
        "outputId": "71f4f8c8-a938-4ad6-cb0f-32e22fadb895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8c601e3b9b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.45.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzXI0Afbonh7",
        "outputId": "94b9f7d0-4102-46eb-b54b-e23808d9d396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tqdm==4.45.0\n",
            "  Using cached tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
            "Installing collected packages: tqdm\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.45.0 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.18.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tqdm-4.45.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJ5Q6w2IFHA"
      },
      "source": [
        "**Ссылка**, на источник текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64uxt-07IEec"
      },
      "source": [
        "DATA_URL = \"http://az.lib.ru/t/tolstoj_a_k/text_0180.shtml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwrBkeLnHuA1"
      },
      "source": [
        "Устанавливаем библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Fr5swwYfz5"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip -q install rnnmorph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbxMKqhPH1Dk"
      },
      "source": [
        "Создаём объект морфологического анализатора `RNNMorph`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24zMUhvi99AV"
      },
      "source": [
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "predictor = RNNMorphPredictor(language=\"ru\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59q1L9p0H9K9"
      },
      "source": [
        "Скачиваем текст, по которому будет дано задание, с помощью `urllib`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uW0fw_h-Pft"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "opener = urllib.request.URLopener({})\n",
        "resource = opener.open(DATA_URL)\n",
        "raw_text = resource.read().decode(resource.headers.get_content_charset()) #Текс с html тегами"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-hSPOjDo4sdh",
        "outputId": "1ef6467c-cb31-45a2-e0fe-44c3a6b59871"
      },
      "source": [
        "raw_text[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<html>\\r\\n<head>\\r\\n<title>Lib.ru/Классика: Толстой Алексей Константинович. Семья вурдалака</title>\\r\\n</head>\\r\\n\\r\\n<body>\\r\\n\\r\\n\\r\\n<center>\\r\\n\\r\\n<h2><a href=/t/tolstoj_a_k/>Толстой Алексей Константинович</a><br>\\r\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZiLHQNSITAt"
      },
      "source": [
        "Как видно, текст содержит html теги, от которых нужно избавиться. Выбрасываем из текста HTML-теги с помощью библиотеки Beatiful soap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We4LkyUMPfuq"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(raw_text, features=\"html.parser\")\n",
        "\n",
        "# kill all script and style elements\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "cleaned_text = soup.get_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lOD8PJnG4rbl",
        "outputId": "f73961b2-8ec9-4e6e-b7c6-4209f0c8e1a3"
      },
      "source": [
        "cleaned_text[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nLib.ru/Классика: Толстой Алексей Константинович. Семья вурдалака\\n\\n\\n\\nТолстой Алексей Константинович\\r\\nСемья вурдалака\\n\\n\\nLib.ru/Классика:\\n\\r\\n\\n\\n[Регистрация]\\n \\n\\r\\n\\r\\n\\r\\n[Найти] \\r\\n[Рейтинги]\\r\\n[Обсуждения]\\r\\n['"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fYYb5hIpnY"
      },
      "source": [
        "С помощью библиотеки [NLTK](https://nltk.org/) разбиваем текст на предложения и токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "hRNu7jPvN6G_",
        "outputId": "2ec8c914-e4a4-4bed-9b47-326d2e18a1be"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(cleaned_text)]\n",
        "\"A total of %d 'sentences'\" % len(tokenized_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A total of 576 'sentences'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRU4KEBAIyYT"
      },
      "source": [
        "## Задание 1\n",
        "С помощью метода `str.isalpha` из стандартной библиотеки Python модифицируйте нижеследующий код так, чтобы в predictions остались только буквенные токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U5HH2CDPVUM",
        "outputId": "713f87b1-b0d4-4289-cfba-43f1de83f654"
      },
      "source": [
        "from tqdm import tqdm\n",
        "predictions = [[pred.normal_form for pred in sent]\n",
        "               for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), \"sentences\") ]\n",
        "predictions[-11:-10] #Сейчас видно, что токены типа \"точка\", \"запятая\" и тд пока присутствуют в предложениях. От них нужно избавиться"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 3s 98ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sentences: 100%|██████████| 576/576 [00:00<00:00, 185540.21it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['другой',\n",
              "  'ребёнок',\n",
              "  'я',\n",
              "  'такой',\n",
              "  'же',\n",
              "  'образ',\n",
              "  'кинуть',\n",
              "  'вслед',\n",
              "  ',',\n",
              "  'но',\n",
              "  'он',\n",
              "  'упасть',\n",
              "  'прямо',\n",
              "  'под',\n",
              "  'копыто',\n",
              "  'лошадь',\n",
              "  'и',\n",
              "  'быть',\n",
              "  'раздавленный',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "predictions = [[pred.normal_form for pred in sent if isinstance(pred.normal_form, str) and pred.normal_form.isalpha()]\n",
        "               for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), \"sentences\")]\n",
        "\n",
        "predictions[-11:-10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw91xpusfVHa",
        "outputId": "cad28bd0-0069-4c0b-ffc1-4f5fa1402a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 1s 94ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sentences: 100%|██████████| 576/576 [00:00<00:00, 110114.82it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['другой',\n",
              "  'ребёнок',\n",
              "  'я',\n",
              "  'такой',\n",
              "  'же',\n",
              "  'образ',\n",
              "  'кинуть',\n",
              "  'вслед',\n",
              "  'но',\n",
              "  'он',\n",
              "  'упасть',\n",
              "  'прямо',\n",
              "  'под',\n",
              "  'копыто',\n",
              "  'лошадь',\n",
              "  'и',\n",
              "  'быть',\n",
              "  'раздавленный']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGDhxhNJtTz"
      },
      "source": [
        "Проверьте себя. Должны получиться следующие значения:\n",
        "\n",
        "*   Предложений: 577 (возможны расхождения в несколько предложений)\n",
        "*   Токенов: примерно 8621 (возможны расхождения в некоторое количество токенов)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwK_qRbw6sac",
        "outputId": "2cfc22d6-3d7a-4ff0-f197-8d2d75ff918d"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "576"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5jL4sWyKUnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d07dbca-8b0a-4e2d-d8fc-c6ec7a922e96"
      },
      "source": [
        "non_uniq_tokens = [word for sentence in predictions for word in sentence]\n",
        "len(non_uniq_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8616"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg2e-1hAKiT3"
      },
      "source": [
        "Для продолжения работы над заданием числа должны быть близки к указанным"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mci9Nd5hKuJP"
      },
      "source": [
        "## Задание 2\n",
        "\n",
        "Используя `non_uniq_tokens`, стоп-слова для русского языка из библиотеки nltk (`nltk.corpus.stopwords`) и `nltk.FreqDist`, вычислите, **какую долю среди 100 самых частотных** токенов в произведении занимают токены, **не относящиеся** к стоп словам.\n",
        "\n",
        "**Например**, если среди 100 самых частотных слов встречается 25 слов, входящих в стоп лист, значит не входят в стоп лист 75 слов, и их доля составит 0.75.\n",
        "\n",
        "**Не бойтесь использовать документацию NLTK и тьюториалы.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHbtLqkLKfZC",
        "outputId": "5f62c17c-d14a-464f-dbe2-b854e66765df"
      },
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = set(stopwords.words(\"russian\"))\n",
        "stopwords.words(\"russian\")[:5] #Пример стоп слов"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и', 'в', 'во', 'не', 'что']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"russian\"))\n",
        "non_uniq_tokens = [word for sentence in predictions for word in sentence]  # список токенов, для которых нужно посчитать долю не стоп-слов\n",
        "\n",
        "# создаем частотный словарь\n",
        "fdist = FreqDist(non_uniq_tokens)\n",
        "\n",
        "# получаем 100 самых частотных токенов\n",
        "most_common_tokens = fdist.most_common(100)\n",
        "\n",
        "# удаляем стоп-слова\n",
        "most_common_tokens_without_stopwords = [token for token in most_common_tokens if token[0] not in STOPWORDS]\n",
        "\n",
        "# вычисляем долю токенов, не являющихся стоп-словами\n",
        "proportion = len(most_common_tokens_without_stopwords) / len(most_common_tokens)\n",
        "\n",
        "print(proportion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjbAORa5ukWC",
        "outputId": "51cff443-70f8-49db-826b-b17964e1cc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdbB95YwtSl"
      },
      "source": [
        "Проверьте себя: должно получиться 0.49 (допустимо небольшое расхождение)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HChyAdk2Ovx1"
      },
      "source": [
        "## Задание 3\n",
        "Вычислите, сколько токенов встречается в тексте **строго больше** 50 раз."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "\n",
        "text_tokens = predictions\n",
        "# Предположим, что у нас есть список токенов text_tokens\n",
        "fdist = FreqDist(text_tokens)\n",
        "\n",
        "# Получаем список наиболее часто встречающихся токенов и их частотность\n",
        "most_common_tokens = fdist.most_common(100)\n",
        "\n",
        "# Создаем множество токенов, которые встречаются более 50 раз\n",
        "tokens_over_50 = {token for token, freq in most_common_tokens if freq > 50}\n",
        "\n",
        "# Подсчитываем количество токенов, входящих в множество tokens_over_50\n",
        "count = sum(1 for token in text_tokens if token in tokens_over_50)\n",
        "\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "86C3OThzvxGu",
        "outputId": "ada09e12-28c2-420d-dc5a-0691cc044393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-7bc9bda07b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Предположим, что у нас есть список токенов text_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Получаем список наиболее часто встречающихся токенов и их частотность\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    550\u001b[0m         '''\n\u001b[1;32m    551\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/probability.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \"\"\"\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Загружаем текст\n",
        "# with open('text.txt', 'r', encoding='utf-8') as f:\n",
        "#    text = f.read()\n",
        "\n",
        "# Токенизируем текст\n",
        "tokens = nltk.word_tokenize(cleaned_text)\n",
        "\n",
        "# Вычисляем частотное распределение токенов\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Выбираем токены, которые встречаются более 50 раз\n",
        "tokens_gt_50 = [token for token, freq in freq_dist.items() if freq > 50]\n",
        "\n",
        "# Выводим количество таких токенов\n",
        "print(len(tokens_gt_50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdXpjZ3B6W7r",
        "outputId": "c514febe-a98b-4d03-d27a-c7b92af4dd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6HZ2w3yxJEh"
      },
      "source": [
        "Проверьте себя: должно получиться значение 22 (возможно небольшое расхождение)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL2 = \"http://az.lib.ru/n/nekrasow_n_a/text_1840_pevitza.shtml\""
      ],
      "metadata": {
        "id": "jP_mleMKyOZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opener = urllib.request.URLopener({})\n",
        "resource = opener.open(DATA_URL2)\n",
        "raw_text2 = resource.read().decode(resource.headers.get_content_charset()) #Текс с html тегами"
      ],
      "metadata": {
        "id": "Oyl1DktbyUvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text2[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2ZpmpN0YycQe",
        "outputId": "2efa98ae-0277-4dd7-9753-ec75af15f80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<html>\\r\\n<head>\\r\\n<title>Lib.ru/Классика: Некрасов Николай Алексеевич. Певица</title>\\r\\n</head>\\r\\n\\r\\n<body>\\r\\n\\r\\n\\r\\n<center>\\r\\n\\r\\n<h2><a href=/n/nekrasow_n_a/>Некрасов Николай Алексеевич</a><br>\\r\\nПевица</h2>\\r\\n\\r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup2 = BeautifulSoup(raw_text2, features=\"html.parser\")\n",
        "\n",
        "# kill all script and style elements\n",
        "for script in soup2([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "cleaned_text2 = soup2.get_text()"
      ],
      "metadata": {
        "id": "Ve0AyURBzHVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text2[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8n6BMZt0zMYW",
        "outputId": "167b1f5c-57b5-4197-954d-6b2aabb0ec9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nLib.ru/Классика: Некрасов Николай Алексеевич. Певица\\n\\n\\n\\nНекрасов Николай Алексеевич\\r\\nПевица\\n\\n\\nLib.ru/Классика:\\n\\r\\n\\n\\n[Регистрация]\\n \\n\\r\\n\\r\\n\\r\\n[Найти] \\r\\n[Рейтинги]\\r\\n[Обсуждения]\\r\\n[Новинки]\\r\\n[Обзоры]\\r\\n[Пом'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_sentences2 = [word_tokenize(sentence) for sentence in sent_tokenize(cleaned_text2)]\n",
        "\"A total of %d 'sentences'\" % len(tokenized_sentences2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "22YpOfh-yf7W",
        "outputId": "badfadf0-cefc-42f7-e9e6-a9a21b468968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A total of 911 'sentences'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "predictions2 = [[pred.normal_form for pred in sent if isinstance(pred.normal_form, str) and pred.normal_form.isalpha()]\n",
        "               for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences2), \"sentences\")]\n",
        "\n",
        "predictions2[-11:-10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUjqdyqIy-CG",
        "outputId": "4e45171c-4679-47dc-8312-13f843f22095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 2s 77ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sentences: 100%|██████████| 911/911 [00:00<00:00, 121645.63it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['с']]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(predictions2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eA4OHhq0i1o",
        "outputId": "7f35e8f5-9019-4417-9c4c-581b1bdbbb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "911"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_uniq_tokens2 = [word for sentence in predictions2 for word in sentence]\n",
        "len(non_uniq_tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU8JDV_R0i9G",
        "outputId": "c7e687b4-9d6f-49b0-e6f2-00ac55a9fba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11695"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"russian\"))\n",
        "non_uniq_tokens2 = [word for sentence in predictions2 for word in sentence]  # список токенов, для которых нужно посчитать долю не стоп-слов\n",
        "\n",
        "# создаем частотный словарь\n",
        "fdist = FreqDist(non_uniq_tokens2)\n",
        "\n",
        "# получаем 50 самых частотных токенов\n",
        "most_common_tokens2 = fdist.most_common(50)\n",
        "\n",
        "# удаляем стоп-слова\n",
        "most_common_tokens_without_stopwords2 = [token for token in most_common_tokens2 if token[0] not in STOPWORDS]\n",
        "\n",
        "# вычисляем долю токенов, не являющихся стоп-словами\n",
        "proportion2 = len(most_common_tokens_without_stopwords2) / len(most_common_tokens2)\n",
        "\n",
        "print(proportion2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqj7Xrog1tA-",
        "outputId": "116270a7-0996-4e16-9add-bc38bd20d7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Загружаем текст\n",
        "# with open('text.txt', 'r', encoding='utf-8') as f:\n",
        "#    text = f.read()\n",
        "\n",
        "# Токенизируем текст\n",
        "tokens = nltk.word_tokenize(cleaned_text2)\n",
        "\n",
        "# Вычисляем частотное распределение токенов\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Выбираем токены, которые встречаются более 20 раз\n",
        "tokens_gt_20 = [token for token, freq in freq_dist.items() if freq > 20]\n",
        "\n",
        "# Выводим количество таких токенов\n",
        "print(len(tokens_gt_20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR5cyOg43g-S",
        "outputId": "5d86cbb4-ff01-41be-b290-ef1d880db43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sssss"
      ],
      "metadata": {
        "id": "zSPbCeYC4PKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from deepfake_detection import DeepFakeDetection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "TiGNfvFTgdkS",
        "outputId": "11fbbd9e-a237-4b75-bb91-eaf4e98d7b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-80cd35bab386>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepfake_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepFakeDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepfake_detection'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained deepfake detection model\n",
        "model = DeepFakeDetection()"
      ],
      "metadata": {
        "id": "rgvdhOFIgdsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyze a video for deepfake\n",
        "def analyze_video(video_path):\n",
        "    # Open the video file\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Initialize variables for frame count and deepfake count\n",
        "    frame_count = 0\n",
        "    deepfake_count = 0\n",
        "\n",
        "    while video.isOpened():\n",
        "        # Read the current frame\n",
        "        ret, frame = video.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert the frame to RGB format\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Increment frame count\n",
        "        frame_count += 1\n",
        "\n",
        "        # Apply deepfake detection to the frame\n",
        "        is_deepfake = model.detect(frame_rgb)\n",
        "\n",
        "        if is_deepfake:\n",
        "            deepfake_count += 1\n",
        "\n",
        "        # Display the frame with detection result\n",
        "        cv2.imshow('Frame', frame)\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "    # Release the video capture object\n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # Print analysis results\n",
        "    print(f\"Total Frames: {frame_count}\")\n",
        "    print(f\"Deepfake Frames: {deepfake_count}\")\n",
        "    print(f\"Deepfake Probability: {deepfake_count / frame_count * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "p1cP4BUSga3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the video\n",
        "analyze_video('path_to_video_file.mp4')"
      ],
      "metadata": {
        "id": "19CSgXthgjSh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}